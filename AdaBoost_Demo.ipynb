{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AdaBoost Demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DHNGUYEN68/TestGithub/blob/master/AdaBoost_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oUsvuQbydAJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4PUlFIQ9sS3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "AdaBoost demo\n",
        "Source : https://www.hdm-stuttgart.de/~maucher/Python/ComputerVision/html/Adaboost.html\n",
        "AdaBoost (Adaptive Boosting) is a Meta Machine Learning algorithm. It defines a procedure to learn and combine a set of weak classifiers to form a strong classifier. The AdaBoost learning procedure consist of M iterations. In each iteration a weak classifier is learned using an appropriate Machine Learning algorithm. In each iteration the training samples that are classified correctly obtain a smaller weight, whereas the weight of all trainingsamples that are not correctly classified remain the same. The effect is that in the training procedure of the next iteration, the training samples that have been classified correctly, have less influence than the not correctly classified training samples. Thus the model in the next iteration is more adapted to the previously misclassified samples. The final classifier is a linear combination of all M weak classifiers. The coefficients in this linear combination are also determined in the iterations: Weak classifiers with a small error rate obtain a larger weight, than weak classifiers with a high error rate.\n",
        "\n",
        "AdaBoost is applied e.g. in the Viola-Jones face detector. Here, in each iteration a weak classifier is learned that just applies a threshold on a single feature. The learned weak classifier is the single feature with a corresponding threshold, that best discriminates positive (face subwindows) from negative samples (non-face subwindows). Note that this optimal single feature and the corresponding threshold depends on the varying weights of the training samples. AdaBoost in Viola-Jones face detection thus not only learns a classifier but also the most informative features out of a very large set of features"
      ]
    },
    {
      "metadata": {
        "id": "-3IOSEw4sTbC",
        "colab_type": "code",
        "outputId": "7871ce3b-68db-419e-9811-a4eae78b781e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "X=np.array([[3,5,2],\n",
        "            [5,5,5],\n",
        "            [3,0,2],\n",
        "            [3,4,1],\n",
        "            [5,0,2],\n",
        "            [2,1,3],\n",
        "            [4,3,1],\n",
        "            [2,5,1],\n",
        "            [0,2,1],\n",
        "            [2,1,5]])\n",
        "\n",
        "print (X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-1ZWdqsag8wU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following example AdaBoost is applied to a set of 10 training samples, which are separated into 2 classes. Each sample is described by 3 features. The training samples are defined in matrix X, the corresponding class labels are defined in the vector C. Inititally all training samples obtain the same weight w=1/10. The weights are stored in variable W. The number of iterations (i.e. the number of weak classifiers) is defined to be M=5.:"
      ]
    },
    {
      "metadata": {
        "id": "P6OVn3V2dK6o",
        "colab_type": "code",
        "outputId": "35102203-0ae8-4675-f74f-cef0aabd1c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "C=np.array([0,0,0,0,0,1,1,1,1,1]) # class labels\n",
        "print (C.shape)\n",
        "print (C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10,)\n",
            "[0 0 0 0 0 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TIy003UyeMTR",
        "colab_type": "code",
        "outputId": "12067fa0-9949-4dd0-a959-850c780ea199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "C_bis = np.atleast_2d(C)\n",
        "print (C_bis.shape)\n",
        "print (C_bis)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 10)\n",
            "[[0 0 0 0 0 1 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ioVG5WQCdp7v",
        "colab_type": "code",
        "outputId": "d6e39b64-4b22-48f6-adc1-5ec8c5791b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# concatenate the labels into the training data\n",
        "print (\"Example data - 3 features, last column is label\")\n",
        "print (np.hstack((X,np.transpose(np.atleast_2d(C)))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example data - 3 features, last column is label\n",
            "[[3 5 2 0]\n",
            " [5 5 5 0]\n",
            " [3 0 2 0]\n",
            " [3 4 1 0]\n",
            " [5 0 2 0]\n",
            " [2 1 3 1]\n",
            " [4 3 1 1]\n",
            " [2 5 1 1]\n",
            " [0 2 1 1]\n",
            " [2 1 5 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QvI9Servfd-k",
        "colab_type": "code",
        "outputId": "eee1d83c-9810-43aa-f982-33af42de4457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "W=1.0/len(C)*np.ones(C.shape[0])\n",
        "print (\"Initial weights\")\n",
        "print (W)\n",
        "M=5 #Number of iterations"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights\n",
            "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_q_0DSc0hXHB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In each iteration of the following loop a weak classifier is learned. This is achieved by determining for each of the three features the threshold, that best separates the negative (class label =0) from the positive (class label = 1) training samples. The learning algorithm applied for this is the LogisticRegression module form scikit-learn. \n",
        "\n",
        "Once the best feature and corresponding threshold of the current iteration is determined, the weighted error J on the training data is applied to compute the variables beta and alpha. Variable beta is applied to adapt the weights of the training samples for the next iteration. \n",
        "\n",
        "Variable alpha (voting power)is the coefficient that determines how strong the currently learned weak classifier contributes to the final classifier.:"
      ]
    },
    {
      "metadata": {
        "id": "1ZORcuQfhR5i",
        "colab_type": "code",
        "outputId": "3af65bea-7550-4c95-80ef-8254ea291a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3162
        }
      },
      "cell_type": "code",
      "source": [
        "# From sci-kit learn\n",
        "reg=linear_model.LogisticRegression()\n",
        "\n",
        "# Array to store \n",
        "# 1.)best feature, \n",
        "# 2.)threshold, \n",
        "# 3.)alpha of best classifier in each iteration\n",
        "bestClassifier=np.zeros((M,3)) \n",
        "\n",
        "for F in range(M):\n",
        "        print (\"\\n\\n\")\n",
        "        print (\"#\"*30)\n",
        "        print (\"Start of Iteration %d\"%F)\n",
        "        IndicatorList=[]\n",
        "        betaList=[]\n",
        "        thresholdList=[]\n",
        "        bestVal=10\n",
        "        bestFeat=10\n",
        "        for i in range(X.shape[1]):\n",
        "                print (\"\\nClassification with respect to feature %d\"%i)\n",
        "                indata= np.transpose(np.atleast_2d(X[:,i]))\n",
        "                reg.fit(indata,C)\n",
        "                threshold=-1*reg.intercept_/reg.coef_\n",
        "                print (\"Threshold = \",threshold)\n",
        "                CE=reg.predict(indata)\n",
        "                print (\"target :     \",C)\n",
        "                print (\"predicted :  \",CE)\n",
        "                Indicator = np.abs(CE-C)\n",
        "                errors=np.dot(Indicator,W)\n",
        "                print (\"J =\",errors)\n",
        "                if errors < bestVal:\n",
        "                        bestVal=errors\n",
        "                        bestFeat=i\n",
        "                J=errors\n",
        "                beta=1.0*J/(1-J)\n",
        "                IndicatorList.append(Indicator)\n",
        "                betaList.append(beta)\n",
        "                thresholdList.append(threshold)\n",
        "                print ('-'*30)\n",
        "\n",
        "        I=IndicatorList[bestFeat]\n",
        "        beta= betaList[bestFeat]\n",
        "        threshold=thresholdList[bestFeat]\n",
        "        alpha=np.log(1.0/beta)\n",
        "        print (\"BestFeature:          \",bestFeat)\n",
        "        print (\"Threshold =           \",threshold)\n",
        "        print (\"Best Beta =           \",beta)\n",
        "        print (\"Corresponding alpha = \",alpha)\n",
        "        bestClassifier[F,:]=np.array([bestFeat,threshold,alpha])\n",
        "        for r in range(C.shape[0]):\n",
        "                W[r]=W[r]*np.power(beta,1-I[r])\n",
        "        Wnorm=np.sum(W)\n",
        "        W=W/Wnorm\n",
        "        print (\"New weights for training samples: \")\n",
        "        print (W)\n",
        "        print (\"#\"*30)\n",
        "        print (\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "##############################\n",
            "Start of Iteration 0\n",
            "\n",
            "Classification with respect to feature 0\n",
            "Threshold =  [[2.02868822]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 0 0 1 0 1 1 1]\n",
            "J = 0.1\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 1\n",
            "Threshold =  [[1.85487592]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 1 0 1 1 0 0 0 1]\n",
            "J = 0.5\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 2\n",
            "Threshold =  [[1.64177335]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 1 0 0 1 1 1 0]\n",
            "J = 0.30000000000000004\n",
            "------------------------------\n",
            "BestFeature:           0\n",
            "Threshold =            [[2.02868822]]\n",
            "Best Beta =            0.11111111111111112\n",
            "Corresponding alpha =  2.1972245773362196\n",
            "New weights for training samples: \n",
            "[0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            " 0.5        0.05555556 0.05555556 0.05555556]\n",
            "##############################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##############################\n",
            "Start of Iteration 1\n",
            "\n",
            "Classification with respect to feature 0\n",
            "Threshold =  [[2.02868822]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 0 0 1 0 1 1 1]\n",
            "J = 0.4999999999999999\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 1\n",
            "Threshold =  [[1.85487592]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 1 0 1 1 0 0 0 1]\n",
            "J = 0.7222222222222221\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 2\n",
            "Threshold =  [[1.64177335]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 1 0 0 1 1 1 0]\n",
            "J = 0.16666666666666663\n",
            "------------------------------\n",
            "BestFeature:           2\n",
            "Threshold =            [[1.64177335]]\n",
            "Best Beta =            0.19999999999999996\n",
            "Corresponding alpha =  1.6094379124341005\n",
            "New weights for training samples: \n",
            "[0.03333333 0.03333333 0.03333333 0.16666667 0.03333333 0.16666667\n",
            " 0.3        0.03333333 0.03333333 0.16666667]\n",
            "##############################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##############################\n",
            "Start of Iteration 2\n",
            "\n",
            "Classification with respect to feature 0\n",
            "Threshold =  [[2.02868822]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 0 0 1 0 1 1 1]\n",
            "J = 0.3\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 1\n",
            "Threshold =  [[1.85487592]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 1 0 1 1 0 0 0 1]\n",
            "J = 0.4333333333333333\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 2\n",
            "Threshold =  [[1.64177335]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 1 0 0 1 1 1 0]\n",
            "J = 0.5000000000000001\n",
            "------------------------------\n",
            "BestFeature:           0\n",
            "Threshold =            [[2.02868822]]\n",
            "Best Beta =            0.4285714285714286\n",
            "Corresponding alpha =  0.8472978603872034\n",
            "New weights for training samples: \n",
            "[0.02380952 0.02380952 0.02380952 0.11904762 0.02380952 0.11904762\n",
            " 0.5        0.02380952 0.02380952 0.11904762]\n",
            "##############################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##############################\n",
            "Start of Iteration 3\n",
            "\n",
            "Classification with respect to feature 0\n",
            "Threshold =  [[2.02868822]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 0 0 1 0 1 1 1]\n",
            "J = 0.4999999999999999\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 1\n",
            "Threshold =  [[1.85487592]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 1 0 1 1 0 0 0 1]\n",
            "J = 0.5952380952380951\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 2\n",
            "Threshold =  [[1.64177335]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 1 0 0 1 1 1 0]\n",
            "J = 0.3571428571428572\n",
            "------------------------------\n",
            "BestFeature:           2\n",
            "Threshold =            [[1.64177335]]\n",
            "Best Beta =            0.5555555555555557\n",
            "Corresponding alpha =  0.5877866649021188\n",
            "New weights for training samples: \n",
            "[0.01851852 0.01851852 0.01851852 0.16666667 0.01851852 0.16666667\n",
            " 0.38888889 0.01851852 0.01851852 0.16666667]\n",
            "##############################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "##############################\n",
            "Start of Iteration 4\n",
            "\n",
            "Classification with respect to feature 0\n",
            "Threshold =  [[2.02868822]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 0 0 1 0 1 1 1]\n",
            "J = 0.38888888888888884\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 1\n",
            "Threshold =  [[1.85487592]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 1 0 1 1 0 0 0 1]\n",
            "J = 0.4629629629629629\n",
            "------------------------------\n",
            "\n",
            "Classification with respect to feature 2\n",
            "Threshold =  [[1.64177335]]\n",
            "target :      [0 0 0 0 0 1 1 1 1 1]\n",
            "predicted :   [0 0 0 1 0 0 1 1 1 0]\n",
            "J = 0.5\n",
            "------------------------------\n",
            "BestFeature:           0\n",
            "Threshold =            [[2.02868822]]\n",
            "Best Beta =            0.6363636363636362\n",
            "Corresponding alpha =  0.4519851237430574\n",
            "New weights for training samples: \n",
            "[0.01515152 0.01515152 0.01515152 0.13636364 0.01515152 0.13636364\n",
            " 0.5        0.01515152 0.01515152 0.13636364]\n",
            "##############################\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sP_qM1V8kqe4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this stage all M=5 weak classifiers are learned. The parameters feature index, threshold and alpha of each weak classifier are stored in the numpy-array bestClassifiers. These parameters uniquelly define the final strong classifier. The strong classifier is finally applied to three different input vectors, defined in the array newdata. The decision criteria is: If the linear combination of the strong classifiers is larger than half of the sum over all 5 alpha-values, then the predicted class label is 0, otherwise it is 1.:"
      ]
    },
    {
      "metadata": {
        "id": "IJciZQa4hSNV",
        "colab_type": "code",
        "outputId": "203c2270-ad67-471d-c759-80c10b363f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "print (\"*\"*50)\n",
        "print (\"Best Classifiers in each iteration:\")\n",
        "print (\"Feature|Threshold|Alpha\")\n",
        "print (bestClassifier)\n",
        "halfSumAlpha=0.5*np.sum(bestClassifier[:,2])\n",
        "print (\"Threshold of combined Classifier  :  \",halfSumAlpha)\n",
        "newdata=np.array([[1,5,1],[2,10,2],[5,3,1]])\n",
        "for u in range(newdata.shape[0]):\n",
        "        data=newdata[u,:]\n",
        "        sum=0\n",
        "        for v in range(M):\n",
        "                if newdata[u,math.floor(bestClassifier[v,0])] > bestClassifier[v,1]:\n",
        "                        sum=sum+bestClassifier[v,2]\n",
        "        print (\"Data : \",newdata[u,:])\n",
        "        if sum > halfSumAlpha:\n",
        "                print (\"Classified as Class 0\")\n",
        "        else:\n",
        "                print (\"Classified as Class 1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Best Classifiers in each iteration:\n",
            "Feature|Threshold|Alpha\n",
            "[[0.         2.02868822 2.19722458]\n",
            " [2.         1.64177335 1.60943791]\n",
            " [0.         2.02868822 0.84729786]\n",
            " [2.         1.64177335 0.58778666]\n",
            " [0.         2.02868822 0.45198512]]\n",
            "Threshold of combined Classifier  :   2.84686606940135\n",
            "Data :  [1 5 1]\n",
            "Classified as Class 1\n",
            "Data :  [ 2 10  2]\n",
            "Classified as Class 1\n",
            "Data :  [5 3 1]\n",
            "Classified as Class 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}